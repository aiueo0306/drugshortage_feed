from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

def generate_rss(items, output_path):
    fg = FeedGenerator()
    fg.title("DSJPï½œåŒ»ç™‚ç”¨åŒ»è–¬å“ä¾›çµ¦çŠ¶æ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")
    fg.link(href="https://drugshortage.jp/updatehistory.php")
    fg.description("DrugShortage.jpã®æ›´æ–°å±¥æ­´")
    fg.language("ja")

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])
        entry.guid(item['link'], permalink=False)
        entry.pubDate(datetime.now(timezone.utc))  # â† ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãã§ä¿®æ­£æ¸ˆã¿

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)

with sync_playwright() as p:
    print("â–¶ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
    browser = p.chromium.launch(headless=True)  # GUIãªã—ã§å®Ÿè¡Œ
    context = browser.new_context(
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    page = context.new_page()

    try:
        print("â–¶ ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        page.goto("https://drugshortage.jp/updatehistory.php", timeout=30000)
        page.wait_for_load_state("load", timeout=30000)
    except PlaywrightTimeoutError:
        print("âš  ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Cloudflareã«ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        browser.close()
        exit()

    print("âœ… Cloudflareé€šéå¾Œã€è¦ç´„åŒæ„ã‚’ã‚»ãƒƒãƒˆä¸­...")
    page.evaluate("localStorage.setItem('policyAgreement', 'true');")
    page.reload()
    page.wait_for_load_state("load", timeout=30000)

    print("â–¶ åŒ»è–¬å“æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    containers = page.locator("body > div > div > div > div > div > div > div > div > div")
    items = []

    count = containers.count()
    print(f"ğŸ“¦ ç™ºè¦‹ã—ãŸé …ç›®æ•°: {count}")

    for i in range(count):
        container = containers.nth(i)
        try:
            title = container.locator("div:nth-child(2) a").inner_text().strip()
            link = container.locator("div:nth-child(2) a").get_attribute("href")
            description = container.locator("div:nth-child(1) div:nth-child(1)").inner_text().strip()
            if link and not link.startswith("http"):
                link = f"https://drugshortage.jp/{link}"
            items.append({"title": title, "link": link, "description": description})
        except:
            continue

    if not items:
        print("âš  æŠ½å‡ºã§ããŸæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ å¤‰æ›´ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    today = datetime.now().strftime("%Y%m%d")
    rss_path = f"rss_output/drugshortage.xml"
    generate_rss(items, rss_path)

    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼\nğŸ“„ ä¿å­˜å…ˆ: {rss_path}")
    browser.close()
